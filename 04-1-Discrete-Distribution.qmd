```{r, output=FALSE, warning=FALSE}
library(tidyverse)
```

# Discrete Distributions (Review)

```{r, echo=FALSE, warning=FALSE, output=FALSE}
px <- read_csv("https://raw.githubusercontent.com/febse/data/main/econ/prob_review/px.csv")
py <- read_csv("https://raw.githubusercontent.com/febse/data/main/econ/prob_review/py.csv")
pxy <- read_csv("https://raw.githubusercontent.com/febse/data/main/econ/prob_review/pxy.csv") %>%
  select(x, y, p)
```

$$
p_X(x) = \begin{cases}
0.250 & \text{für } x = 0\\
0.095 & \text{für } x = 1 \\
0.272 & \text{für } x = 2 \\
0.383 & \text{für } x = 3 \\
0 & \text{sonst}
\end{cases}
$$

$$
p_Y(y) = \begin{cases}
0.76 & \text{für } y = 2\\
0.24 & \text{für } y = 3 \\
0 & \text{sonst}
\end{cases}
$$

```{r}
#| label: tbl-distr-x
#| tbl-cap: "Probability mass function of $X$."
 
px %>%
  knitr::kable()
```

```{r}
#| label: tbl-distr-y
#| tbl-cap: "Probability mass function of $Y$"
 
py %>%
  knitr::kable()
```

The usual visualization of a probability mass function is a bar plot.

```{r}
px %>%
  ggplot(aes(x = x, y = p)) + 
    geom_col() + 
    labs(
      x = "Ausgang",
      y = "Wahrscheinlichkeit"
    )
```


$$
\sum_{x = 0}^{3}p_X(x) = 0.250 + 0.095 + 0.272 + 0.383 = 1
$$

::: {#exr-prob-y}
## Wahrscheinlichkeitsfunktion von $Y$

Benuzten Sie die `sum` Funktion um zu zeigen, dass die Wahrscheinlichkeitsfunktion von $Y$ zu eins summiert.

```{r}

```
:::

::: {#exm-sampling-x}
## Zufällige Stichproben von $X$

```{r}
set.seed(12)

smpl_x <- px %>%
  slice_sample(n = 20, weight_by = p, replace = TRUE)

head(smpl_x) %>%
  knitr::kable(digits=3)
```

Wir können die beobachteten Häufigkeiten der Ausgänge visualisieren.

```{r}
# Absolute frequencies (counts)
table(smpl_x$x)

# Relative frequencies (shares)
table(smpl_x$x) / nrow(smpl_x)
```

```{r}
smpl_x %>%
  ggplot(aes(x = x)) + 
    geom_bar() + 
    labs(
      x = "Outcome",
      y = "Frequency"
    )
```
:::

::: {#exr-prob-y-sample}
## Zufallsstichproben von $Y$

Wählen Sie 2000 Stichproben aus der Verteilung von $Y$ und visualisieren Sie die Häufigkeiten.


```{r}
# Take the samples here

```

```{r}
# Visualize the frequencies here

```
:::

## Der Erwartungswert

Der Erwartungswert einer Zufallsvariable ist der gewichtete Durchschnitt der möglichen Werte, die diese Zufallsvariable annehmen kann.

$$
\begin{align}
\mu_x & = E(X) = \sum_{x = 0}^{3} x p_X(x) = 0 \times 0.250 + 1 \times 0.095 + 2 \times 0.272 + 3 \times 0.383 = 1.788 \\
\end{align}
$$

```{r}
mu_x <- sum(px$x * px$p)
mu_x

mean(smpl_x$x)
```

```{r}
px %>%
  ggplot(aes(x = x, y = p)) + 
    geom_col() + 
    labs(
      x = "Ausgang",
      y = "Wahrscheinlichkeit"
    ) + 
    geom_vline(xintercept = mu_x, color = "firebrick")
```

::: {#exr-expectation}
## Der Erwartungswert

Berechnen Sie den Erwartungswert von $Y$.
:::

::: solution
```{r}
# Schreiben Sie Ihren Code hier

```
:::

Der Erwartungswert minimiert den *mittleren (erwarteten) quadratischen Fehler*.

$$
L(X) = (X - \hat{x})^2
$$

$$
E(L(X)) = E[(X - \hat{x})^2]
$$

Lassen Sie uns ein Beispiel konstruieren. Sie müssen das Ergebnis von $X$ vorhersagen und Sie denken, dass die beste Vorhersage $\bar{x} = 1$ ist. Wenn das Spiel läuft, wird es vier mögliche Werte produzieren: 0, 1, 2 und 3. Der Fehler, den Sie machen werden, ist:

$$
L(x) = (x - 1)^2 =
\begin{cases}
   (0 - 1)^2 = 1 & \text{x = 0}\\
   (1 - 1)^2 = 0 & \text{x = 1}\\
   (2 - 1)^2 = 1 & \text{x = 2}\\
   (3 - 1)^2 = 4 & \text{x = 3}
\end{cases}
$$

::: {#exr-expected-loss}
## Der erwartete quadratische Verlust

Berechnen Sie den erwarteten quadratischen Verlust für eine Vorhersage $\bar{x} = 1.5$.

:::

::: solution
```{r}
## Schreiben Sie Ihren Code hier

# px %>%
#   summarise(
#     loss = ?
#   )
```
:::

## Die Varianz

Die Varianz ist der erwartete Wert der quadrierten Abweichungen vom Erwartungswert.

$$
\begin{align}
Var(X) & = \sum_{x = 0}^{3} (x - \mu_x)^2 \times p_X(x) \\
       & = (0 - 1.788)^2 \times 0.250 + (1 - 1.788)^2 \times 0.095 + (2 - 1.788)^2\times 0.272 + (3 - 1.788)^2 \times 0.383 \\
       & = (-1.788)^2 \times 0.250 + (-0.788)^2 \times 0.095 + (0.212)^2\times 0.272 + (1.212)^2 \times 0.383 \\
       & = 3.196 \times 0.250 + 0.620^2 \times 0.095 + 0.044 \times 0.272 + 1.468 \times 0.383 \\
       & \approx 1.433
\end{align}
$$ {#eq-variance}

```{r}
(px$x - mu_x)
(px$x - mu_x)^2
px$p * (px$x - mu_x)^2
sum(px$p * (px$x - mu_x)^2)
```

You can see from @eq-variance that it is the expected value of the squared deviations from the expected value.

$$
Var(X) = E(X - E(X))^2 = \sum_{x} (x - E(X))^2 p_X(x)
$$

::: {#def-variance}
## Die Varianz

Die Varianz einer Zufallsvariable (Verteilung) ist eine Zusammenfassung der Verteilung und beschreibt ihre Streuung: wie unterschiedlich sind die Werte, die diese Verteilung generieren wird.

$$
Var(X) = E[(X - E(X))^2] = E(X^2) - E(X)^2
$$
:::

::: {#exr-variance}
## Variance

Berechnen Sie die Varianz von $Y$.

:::

::: solution
```{r}
# Type your code here

```
:::

::: {#thm-exp-value-props}
## Eigenschaften des Erwartungswertes

Es sei $X$ eine Zufallsvariable mit Erwartungswert $E(X)$, $Y$ eine Zufallsvariable mit Erwartungswert $E(Y)$ und $a$ eine feste Konstante ($a \in \mathbb{R}$). Der Erwartungsweropeartor $E$ hat die folgenden Eigenschaften:

$$
\begin{align}
E(a) & = a \\
E(aX) & = aE(X) \\
E(X + Y) & = E(X) + E(Y)
\end{align}
$$


Ferner gilt, wenn $X$ und $Y$ *unkorreliert* sind, dann gilt für das Produkt der beiden Zufallsvariablen:

$$
E(XY) = E(X)E(Y)
$$
:::

## Die gemeinsame Verteilung

```{r}
pxy %>%
  knitr::kable()
```


```{r}
#| label: tbl-distr-dep
#| tbl-cap: "Die gemeinsame Verteilung von $X$ und $Y$."

pxy %>%
  pivot_wider(
    id_cols = x,
    names_from = y,
    values_from = p,
    names_prefix="y="
    ) %>%
  knitr::kable(digits = 3)
```

$$
p_{XY}(x=2, y=3) = 0.043
$$

$$
\sum_{x = 0}^{3}\sum_{y = 2}^{3} p_{XY}(x, y) = 1
$$

```{r}
sum(pxy$p)
```


## Die Randverteilungen {#sec-marginal-distr}

Die Randverteilung von $X$ erhalten wir, indem wir die gemeinsame Verteilung von $X$ und $Y$ über alle möglichen Werte von $Y$ summieren.

$$
p_X(x) = \sum_{y=2}^{3}p_{XY}(x, y)
$$

```{r}
pxy %>% 
  group_by(x) %>% 
  summarise(p_x = sum(p))
```

::: {#exr-marginal-distr-y}
## Die Randverteilung von $Y$

Berechnen Sie die Randverteilung von $Y$ aus der gemeinsamen Verteilung in @tbl-distr-dep. Verwenden Sie die `pxy` Tabelle und die `group_by` und `summarise` Funktionen.

```{r}
# pxy %>%
#   group_by(...) %>%
#   summarise(p_y = sum(p))
```
:::

## Bedingte Verteilungen

```{r}
#| label: tbl-cond-distr-y-dep-x
#| tbl-cap: "Conditional distributions of $Y$ given $X$"

pxy_w <- pxy %>%
  pivot_wider(
    id_cols = x,
    names_from = y,
    values_from = p,
    names_prefix = "y="
  ) %>%
  mutate(
    p_x = `y=2` + `y=3`,
    `y=2` = `y=2` / p_x,
    `y=3` = `y=3` / p_x
  )

pxy_w %>%
  knitr::kable(digits = 3)
```


## Unabhängigkeit

$$
p_{XY}(x, y) = p_X(x)p_Y(y)
$$

```{r}
pxy_ind <- expand_grid(
  px %>% rename(p_x = p), 
  py %>% rename(p_y = p)
)
pxy_ind <- pxy_ind %>%
  mutate(
    p = p_x * p_y
  )
```

```{r, collapse=TRUE}
#| label: tbl-distr-indep
#| tbl-cap: "Die gemeinsame Verteilung von $X$ und $Y$ unter der Annahme der Unabhängigkeit."

pxy_ind_w <- pxy_ind %>%
  pivot_wider(
    id_cols = x,
    names_from = y,
    values_from = p,
    names_prefix = "y="
  )

pxy_ind_w %>%
  knitr::kable(digits = 3)
```

$$
p_{Y|X}(x, y) = \frac{p_{XY}(x, y)}{p_X(x)}
$$

```{r}
#| label: tbl-xy-distr-ind-w
#| tbl-cap: "Die bedingten Verteilungen von $Y$ gegeben $X$ unter der Annahme der Unabhängigkeit."

pxy_ind_w %>%
  mutate(
    p_x = `y=2` + `y=3`,
    `y=2` = `y=2` / p_x,
    `y=3` = `y=3` / p_x
  ) %>%
  knitr::kable(digits = 3)
```


```{r}
pxy %>%
  ggplot(aes(y = factor(x), x = p, color=factor(y))) +
  geom_col(position="dodge") +
  labs(
    x = "p(x | y)",
    y = "x",
  ) + 
  theme_minimal()
```

## Der bedingte Erwartungswert

$$
E(Y | X=0) = \sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \times 0.76 + 3 \times 0.24 = 2.24
$$

```{r}
2 * 0.76 + 3 * 0.24
```


$$
E(Y | X=0) = \sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \times 0.964 + 3 \times 0.036 = 2.036
$$

```{r}
2 * 0.964 + 3 * 0.036
```


$$
E(Y | X = x) = \begin{cases}
  2.036 & \text{für } x = 0 \\
  2.060 & \text{für } x = 1 \\
  2.158 & \text{für } x = 2 \\
  2.475 & \text{für } x = 3
\end{cases}
$$

```{r}
#| label: tbl-cond-exp-y
#| tbl-cap: "Der bedingte Erwartungswert von $Y$ gegeben $X$."

pxy %>%
  group_by(x) %>%
  mutate(
    p_y_x = p / sum(p)
  ) %>%
  summarize(
    E_Y_given_X = sum(y * p_y_x) 
  ) %>%
  knitr::kable(digits = 3)
```


::: {#exr-cond-exp-ind}
Calculate the expected value of $Y$ given $X$ for every possible value of $X$ in the case joint distribution under independence.
:::

::: {#exm-sampling-joint}
## Zufallsstichproben aus der gemeinsamen Verteilung

```{r}
sample_joint <- pxy %>%
  slice_sample(n = 1000, weight_by = p, replace = TRUE)

head(sample_joint)
```

```{r}
sample_joint %>%
  group_by(x, y) %>%
  summarise(
    p = first(p),
    n = n(),
    f = n / nrow(sample_joint)
  )
```
:::

## Die Kovarianz

Die Kovarianz von zwei Zufallsvariablen $X$ und $Y$ ist ein Maß für die gemeinsame Variation der beiden Zufallsvariablen.

::: {#def-covariance}
## Die Kovarianz

Die Kovarianz zwischen zwei Zufallsvariablen $X$ und $Y$ ist definiert als

$$
Cov(X, Y) = E[(X - E(X))(Y - E(Y))]
$$ Alternatively, it can be computed using the decomposition formula:

$$
Cov(X, Y) = E(XY) - E(X)E(Y)
$$
:::


::: {#def-correlation}
## Die Korrelation

$$
\rho(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}}
$$ 

Die Korrelation ist ein Maß für die lineare Beziehung zwischen zwei Zufallsvariablen. Es ist ein dimensionsloses Maß, das zwischen -1 und 1 liegt.

$$
-1 \leq \rho(X, Y) \leq 1
$$
:::

::: {#exr-corr-proof}
## Die Korrelation

Es sei $X$ eine Zufallsvariable mit $E(X) = 0$ und $Y = a + bX$. Zeigen Sie, dass die Korrelation zwischen $X$ und $Y$ gleich eins oder minus eins ist, abhängig vom Vorzeichen von $b$.
:::

::: {#thm-covariance-props}
## Eigenchaften der Kovarianz

Es seien $X$ und $Y$ Zufallsvariablen und $a, b \in \mathbb{R}$ feste Konstanten. Die Kovarianz hat die folgenden Eigenschaften:

$$
Var(aX + bY) = a^2 Var(X) + b^2Var(Y) + 2abCov(X, Y)
$$
:::

::: {#exr-covariance}
## Covariance

Compute the covariance of $X$ and $Y$ under the joint distributions given in @tbl-distr-indep and @tbl-distr-dep. Use the `pxy` and `pxy_ind` tables for these calculations.
:::

::: solution
```{r}
# Type your code here

```
:::